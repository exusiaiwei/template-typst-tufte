// 导入辅助函数
#import "../template.typ": note, notecite, wideblock

= 数学基础

机器学习建立在坚实的数学基础之上。#note[不要被数学吓倒！我们会从实用的角度介绍这些概念，重点在于理解其在机器学习中的应用。]本章将介绍学习机器学习所必需的数学知识，包括线性代数、概率论、统计学和优化理论。

== 线性代数

线性代数是机器学习的基石，几乎所有的机器学习算法都涉及向量和矩阵运算。

=== 向量和矩阵

向量可以看作是空间中的点或方向。在机器学习中，一个数据样本通常表示为一个向量：

$ bold(x) = mat(x_1; x_2; dots.v; x_n) $

矩阵是向量的推广，可以表示一批数据样本或线性变换。#note[在Python的NumPy库中，向量和矩阵都用`ndarray`对象表示，这使得操作非常便捷。]

=== 矩阵运算

几个重要的矩阵运算：

1. *矩阵乘法*：$(bold(A B))_(i,j) = sum_k A_(i,k) B_(k,j)$

2. *转置*：$(bold(A)^top)_(i,j) = A_(j,i)$

3. *逆矩阵*：$bold(A) bold(A)^(-1) = bold(I)$（如果存在）

4. *特征值和特征向量*：$bold(A) bold(v) = lambda bold(v)$#note[特征值分解在主成分分析（PCA）等降维技术中扮演核心角色。]

```python
import numpy as np

# 创建矩阵
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# 矩阵乘法
C = A @ B

# 特征值和特征向量
eigenvalues, eigenvectors = np.linalg.eig(A)
```

== 概率论

概率论为处理不确定性提供了数学框架，是理解许多机器学习算法的关键。

=== 基本概念

- *随机变量*：其值由随机现象决定的变量#note[区分离散随机变量（如掷骰子）和连续随机变量（如身高）很重要。]
- *概率分布*：描述随机变量取各个可能值的概率
- *期望*：$EE[X] = sum_x x P(X = x)$
- *方差*：$"Var"(X) = EE[(X - EE[X])^2]$

=== 常用分布

*正态分布*（高斯分布）是最重要的概率分布之一：

$ p(x | mu, sigma^2) = 1/(sqrt(2 pi sigma^2)) exp(-(x - mu)^2 / (2 sigma^2)) $

其中 $mu$ 是均值，$sigma^2$ 是方差。

== 优化理论

机器学习的核心任务是找到使某个目标函数（损失函数）最小化的参数。

=== 梯度下降

梯度下降是最基本的优化算法：#note[梯度指向函数增长最快的方向，所以负梯度方向是函数下降最快的方向。]

$ theta_(t+1) = theta_t - alpha nabla L(theta_t) $

其中：
- $theta$ 是模型参数
- $alpha$ 是学习率
- $nabla L(theta)$ 是损失函数的梯度

== 本章小结

本章介绍了机器学习所需的核心数学工具：

- *线性代数*提供了数据表示和变换的语言
- *概率论*帮助我们处理不确定性和建模随机现象
- *优化理论*让我们能够找到最优的模型参数

这些数学工具将在后续章节中反复出现，建议读者在遇到具体应用时再深入理解。
