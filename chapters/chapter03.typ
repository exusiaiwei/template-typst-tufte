// 导入辅助函数
#import "../template.typ": note, notecite, wideblock

= 监督学习

监督学习是机器学习中最常见和最成熟的范式。#note[监督学习之所以称为"监督"，是因为训练过程类似于老师监督学生学习——每个训练样本都有正确答案。]在监督学习中，我们有一组带标签的训练数据，目标是学习从输入到输出的映射关系。

== 问题定义

给定训练数据集 $cal(D) = {(bold(x)_1, y_1), (bold(x)_2, y_2), dots, (bold(x)_n, y_n)}$，其中：

- $bold(x)_i in RR^d$ 是输入特征向量
- $y_i$ 是对应的标签或目标值

监督学习主要分为两类任务：

1. *回归*（Regression）：目标变量是连续值
2. *分类*（Classification）：目标变量是离散类别#note[二分类只有两个类别，多分类有三个或更多类别。]

== 线性回归

线性回归是最简单也是最基础的监督学习算法。

=== 模型定义

线性回归假设目标变量与特征之间存在线性关系：

$ y = theta_0 + theta_1 x_1 + theta_2 x_2 + dots + theta_d x_d + epsilon $

用向量形式表示：

$ y = bold(theta)^top bold(x) + epsilon $

=== 损失函数

我们使用均方误差（MSE）作为损失函数：#note[为什么选择平方误差？因为它是凸函数，有唯一的全局最小值，便于优化。]

$ L(bold(theta)) = 1/n sum_(i=1)^n (y_i - bold(theta)^top bold(x)_i)^2 $

== 逻辑回归

尽管名字中有"回归"，逻辑回归实际上是一种分类算法。#note[逻辑回归是许多复杂模型的基础，理解它对学习深度学习至关重要。]

=== 模型定义

逻辑回归使用sigmoid函数将线性组合映射到(0, 1)区间：

$ P(y=1 | bold(x)) = sigma(bold(theta)^top bold(x)) = 1/(1 + e^(-bold(theta)^top bold(x))) $

#wideblock[
  #table(
    columns: (1.5fr, 2fr, 2fr, 1.5fr),
    table.header[算法][优点][缺点][适用场景],

    [线性回归],
    [简单、可解释],
    [只能建模线性关系],
    [线性相关数据],

    [逻辑回归],
    [概率输出],
    [线性决策边界],
    [二分类问题],

    [决策树],
    [非线性、可解释],
    [容易过拟合],
    [非线性决策边界],
  )
]

== 模型评估

=== 回归指标

- *均方误差*（MSE）：$"MSE" = 1/n sum_(i=1)^n (y_i - hat(y)_i)^2$
- *决定系数*（R²）：$R^2 = 1 - ("SS"_"res")/("SS"_"tot")$

=== 分类指标

- *准确率*（Accuracy）：正确预测的比例
- *精确率*（Precision）：预测为正的样本中真正为正的比例
- *召回率*（Recall）：实际为正的样本中被正确预测的比例#note[RMSE与目标变量单位相同，更容易解释。]

== 本章小结

监督学习是机器学习的核心，本章介绍了：

- 线性回归和逻辑回归两个基础算法
- 不同的损失函数和优化方法
- 模型评估的各种指标

这些概念和技术是理解更复杂模型的基础。
